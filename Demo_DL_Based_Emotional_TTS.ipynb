{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainarsareddy/Assignment/blob/main/Demo_DL_Based_Emotional_TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ds3NJZ5T92"
      },
      "source": [
        "# DL Based Emotional Text to Speech\n",
        "\n",
        "In this demo, we provide an interface to generate emotional speech from user inputs for both the emotional label and the text.\n",
        "\n",
        "The models that are trained are [Tacotron](https://github.com/Emotional-Text-to-Speech/tacotron_pytorch) and [DC-TTS](https://github.com/Emotional-Text-to-Speech/pytorch-dc-tts).\n",
        "\n",
        "Further information about our approaches and *exactly how* did we develop this demo can be seen [here](https://github.com/Emotional-Text-to-Speech/dl-for-emo-tts).\n",
        "\n",
        "---\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7gRpQLXQSID"
      },
      "source": [
        "## Download the required code and install the dependences\n",
        "\n",
        "- Make sure you have clicked on ```Open in Playground``` to be able to run the cells. Set your runtime to ```GPU```. This can be done with the following steps:\n",
        "  - Click on ```Runtime``` on the menubar above\n",
        "  - Select ```Change runtime type```\n",
        "  - Select ```GPU``` from the ```Hardware accelerator``` dropdown and save.\n",
        "- Run the cell below. It will automatically create the required directory structure. In order to run the cell, click on the **arrow** that is on the left column of the cell (hover over the ```[]``` symbol). Optionally, you can also press ```Shift + Enter ```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V4d2LXHbC-Es",
        "outputId": "5d32cd97-6172-4f45-c806-28cbbae7534d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-dc-tts'...\n",
            "remote: Enumerating objects: 1904, done.\u001b[K\n",
            "remote: Counting objects: 100% (170/170), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 1904 (delta 139), reused 129 (delta 129), pack-reused 1734 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1904/1904), 277.55 MiB | 17.12 MiB/s, done.\n",
            "Resolving deltas: 100% (235/235), done.\n",
            "Cloning into 'tacotron_pytorch'...\n",
            "remote: Enumerating objects: 150, done.\u001b[K\n",
            "remote: Total 150 (delta 0), reused 0 (delta 0), pack-reused 150 (from 1)\u001b[K\n",
            "Receiving objects: 100% (150/150), 21.19 MiB | 15.83 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "Submodule 'lib/tacotron' (https://github.com/r9y9/tacotron) registered for path 'lib/tacotron'\n",
            "Cloning into '/content/tacotron_pytorch/lib/tacotron'...\n",
            "remote: Enumerating objects: 217, done.        \n",
            "remote: Counting objects: 100% (5/5), done.        \n",
            "remote: Compressing objects: 100% (5/5), done.        \n",
            "remote: Total 217 (delta 0), reused 0 (delta 0), pack-reused 212 (from 1)        \n",
            "Receiving objects: 100% (217/217), 65.77 KiB | 16.44 MiB/s, done.\n",
            "Resolving deltas: 100% (111/111), done.\n",
            "Submodule path 'lib/tacotron': checked out '0987cedd0d6a6909749c594ca978ac4e11ae79ae'\n",
            "Obtaining file:///content/tacotron_pytorch\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tacotron_pytorch==0.0.1+e6b1a39) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from tacotron_pytorch==0.0.1+e6b1a39) (1.16.1)\n",
            "Installing collected packages: tacotron_pytorch\n",
            "  Running setup.py develop for tacotron_pytorch\n",
            "Successfully installed tacotron_pytorch-0.0.1+e6b1a39\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1QDqQ28-9HKwbk-tvbuxdVgZWqP4esAWi\n",
            "From (redirected): https://drive.google.com/uc?id=1QDqQ28-9HKwbk-tvbuxdVgZWqP4esAWi&confirm=t&uuid=54884810-193e-4370-90b9-8c5ace689301\n",
            "To: /content/trained_models/angry_dctts.pth\n",
            "100%|██████████| 288M/288M [00:02<00:00, 126MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15W26uWXF-OGaA5Kl0uZMBrCwtLA7TSZD\n",
            "To: /content/trained_models/neutral_dctts.pth\n",
            "100%|██████████| 95.9M/95.9M [00:01<00:00, 66.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1SGLBcq57nbNuUjvLauSdpZ82aJxsPhVi\n",
            "From (redirected): https://drive.google.com/uc?id=1SGLBcq57nbNuUjvLauSdpZ82aJxsPhVi&confirm=t&uuid=88e7ace4-3326-450a-8089-2fa7a3a3c682\n",
            "To: /content/trained_models/ssrn.pth\n",
            "100%|██████████| 331M/331M [00:02<00:00, 164MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1noCgRVdyOgpbTu0DYfOHRemyZcTv_EX-\n",
            "To: /content/trained_models/disgust_tacotron.pth\n",
            "100%|██████████| 25.8M/25.8M [00:01<00:00, 13.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vq581_o3eOHPdxziiaIDbDX-QKiqJCCs\n",
            "To: /content/trained_models/amused_tacotron.pth\n",
            "100%|██████████| 68.3M/68.3M [00:01<00:00, 47.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-kytGBfrb-P8BNguS58a87V5fACO9vJU\n",
            "To: /content/trained_models/sleepiness_tacotron.pth\n",
            "100%|██████████| 68.3M/68.3M [00:00<00:00, 93.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trained_models/sleepiness_tacotron.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "! git clone https://github.com/Emotional-Text-to-Speech/pytorch-dc-tts\n",
        "! git clone --recursive https://github.com/Emotional-Text-to-Speech/tacotron_pytorch.git\n",
        "! cd \"tacotron_pytorch/\" && pip install -e .\n",
        "! pip install unidecode\n",
        "! pip install gdown\n",
        "! mkdir trained_models\n",
        "\n",
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1QDqQ28-9HKwbk-tvbuxdVgZWqP4esAWi'\n",
        "output = 'trained_models/angry_dctts.pth'\n",
        "gdown.download(url, output, quiet=False)\n",
        "url = 'https://drive.google.com/uc?id=15W26uWXF-OGaA5Kl0uZMBrCwtLA7TSZD'\n",
        "output = 'trained_models/neutral_dctts.pth'\n",
        "gdown.download(url, output, quiet=False)\n",
        "url = 'https://drive.google.com/uc?id=1SGLBcq57nbNuUjvLauSdpZ82aJxsPhVi'\n",
        "output = 'trained_models/ssrn.pth'\n",
        "gdown.download(url, output, quiet=False)\n",
        "url = 'https://drive.google.com/uc?id=1noCgRVdyOgpbTu0DYfOHRemyZcTv_EX-'\n",
        "output = 'trained_models/disgust_tacotron.pth'\n",
        "gdown.download(url, output, quiet=False)\n",
        "url = 'https://drive.google.com/uc?id=1vq581_o3eOHPdxziiaIDbDX-QKiqJCCs'\n",
        "output = 'trained_models/amused_tacotron.pth'\n",
        "gdown.download(url, output, quiet=False)\n",
        "url = 'https://drive.google.com/uc?id=1-kytGBfrb-P8BNguS58a87V5fACO9vJU'\n",
        "output = 'trained_models/sleepiness_tacotron.pth'\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaZ8INV0IOgH"
      },
      "source": [
        "## Setup the required code\n",
        "\n",
        "- Run the cell below. It will automatically create the required directory structure. In order to run the cell, click on the **arrow** that is on the left column of the cell (hover over the ```[]``` symbol). Optionally, you can also press ```Shift + Enter ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "GnZPVzThIBkd",
        "outputId": "e45fde72-3056-4c04-ee7c-a8a829eaa829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets.emovdb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3327818037.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_to_wav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectrogram2wav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_last_checkpoint_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_checkpoint_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to_png\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memovdb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# For the Tacotron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets.emovdb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "%pylab inline\n",
        "rcParams[\"figure.figsize\"] = (10,5)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "sys.path.append('pytorch-dc-tts/')\n",
        "sys.path.append('pytorch-dc-tts/models')\n",
        "sys.path.append(\"tacotron_pytorch/\")\n",
        "sys.path.append(\"tacotron_pytorch/lib/tacotron\")\n",
        "\n",
        "# For the DC-TTS\n",
        "import torch\n",
        "from text2mel import Text2Mel\n",
        "from ssrn import SSRN\n",
        "from audio import save_to_wav, spectrogram2wav\n",
        "from utils import get_last_checkpoint_file_name, load_checkpoint_test, save_to_png, load_checkpoint\n",
        "from datasets.emovdb import vocab, get_test_data\n",
        "\n",
        "# For the Tacotron\n",
        "from text import text_to_sequence, symbols\n",
        "# from util import audio\n",
        "\n",
        "from tacotron_pytorch import Tacotron\n",
        "from synthesis import tts as _tts\n",
        "\n",
        "# For Audio/Display purposes\n",
        "import librosa.display\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from IPython.display import display\n",
        "from google.colab import widgets\n",
        "from google.colab import output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "text2mel = Text2Mel(vocab).eval()\n",
        "\n",
        "ssrn = SSRN().eval()\n",
        "load_checkpoint('trained_models/ssrn.pth', ssrn, None)\n",
        "\n",
        "model = Tacotron(n_vocab=len(symbols),\n",
        "                 embedding_dim=256,\n",
        "                 mel_dim=80,\n",
        "                 linear_dim=1025,\n",
        "                 r=5,\n",
        "                 padding_idx=None,\n",
        "                 use_memory_mask=False,\n",
        "                 )\n",
        "\n",
        "def visualize(alignment, spectrogram, Emotion):\n",
        "    label_fontsize = 16\n",
        "    tb = widgets.TabBar(['Alignment', 'Spectrogram'], location='top')\n",
        "    with tb.output_to('Alignment'):\n",
        "      imshow(alignment.T, aspect=\"auto\", origin=\"lower\", interpolation=None)\n",
        "      xlabel(\"Decoder timestamp\", fontsize=label_fontsize)\n",
        "      ylabel(\"Encoder timestamp\", fontsize=label_fontsize)\n",
        "    with tb.output_to('Spectrogram'):\n",
        "      if Emotion == 'Disgust' or Emotion == 'Amused' or Emotion == 'Sleepiness':\n",
        "        librosa.display.specshow(spectrogram.T, sr=fs,hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\")\n",
        "      else:\n",
        "        librosa.display.specshow(spectrogram, sr=fs,hop_length=hop_length, x_axis=\"time\", y_axis=\"linear\")\n",
        "\n",
        "      xlabel(\"Time\", fontsize=label_fontsize)\n",
        "      ylabel(\"Hz\", fontsize=label_fontsize)\n",
        "\n",
        "def tts_dctts(text2mel, ssrn, text):\n",
        "  sentences = [text]\n",
        "\n",
        "  max_N = len(text)\n",
        "  L = torch.from_numpy(get_test_data(sentences, max_N))\n",
        "  zeros = torch.from_numpy(np.zeros((1, 80, 1), np.float32))\n",
        "  Y = zeros\n",
        "  A = None\n",
        "\n",
        "  for t in range(210):\n",
        "      _, Y_t, A = text2mel(L, Y, monotonic_attention=True)\n",
        "      Y = torch.cat((zeros, Y_t), -1)\n",
        "      _, attention = torch.max(A[0, :, -1], 0)\n",
        "      attention = attention.item()\n",
        "      if L[0, attention] == vocab.index('E'):  # EOS\n",
        "          break\n",
        "\n",
        "  _, Z = ssrn(Y)\n",
        "  Y = Y.cpu().detach().numpy()\n",
        "  A = A.cpu().detach().numpy()\n",
        "  Z = Z.cpu().detach().numpy()\n",
        "\n",
        "  return spectrogram2wav(Z[0, :, :].T), A[0, :, :], Y[0, :, :]\n",
        "\n",
        "\n",
        "def tts_tacotron(model, text):\n",
        "    waveform, alignment, spectrogram = _tts(model, text)\n",
        "    return waveform, alignment, spectrogram\n",
        "\n",
        "def present(waveform, Emotion, figures=False):\n",
        "  if figures!=False:\n",
        "        visualize(figures[0], figures[1], Emotion)\n",
        "  IPython.display.display(Audio(waveform, rate=fs))\n",
        "\n",
        "\n",
        "fs = 20000 #20000\n",
        "hop_length = 250\n",
        "model.decoder.max_decoder_steps = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiSAxs7XIUO7"
      },
      "source": [
        "## Run the Demo\n",
        "\n",
        "- Select an ```Emotion``` from the dropdown and enter the ```Text``` that you want to be generated.\n",
        "- Run the cell below. It will automatically create the required directory structure. In order to run the cell, click on the **arrow** that is on the left column of the cell (hover over the ```[]``` symbol). Optionally, you can also press ```Shift + Enter ```\n",
        "\n",
        "**Play the speech with the generated audio player and view the required plots by clicking on their respective tabs!**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3jKM6GfzlgpS"
      },
      "outputs": [],
      "source": [
        "#@title Select the emotion and type the text\n",
        "\n",
        "%pylab inline\n",
        "\n",
        "Emotion = \"Neutral\" #@param [\"Neutral\", \"Angry\", \"Disgust\", \"Sleepiness\", \"Amused\"]\n",
        "Text = 'I am exhausted.' #@param {type:\"string\"}\n",
        "\n",
        "wav, align, mel = None, None, None\n",
        "\n",
        "if Emotion == \"Neutral\":\n",
        "  load_checkpoint('trained_models/'+Emotion.lower()+'_dctts.pth', text2mel, None)\n",
        "  wav, align, mel = tts_dctts(text2mel, ssrn, Text)\n",
        "elif Emotion == \"Angry\":\n",
        "  load_checkpoint_test('trained_models/'+Emotion.lower()+'_dctts.pth', text2mel, None)\n",
        "  wav, align, mel = tts_dctts(text2mel, ssrn, Text)\n",
        "  # wav = wav.T\n",
        "elif Emotion == \"Disgust\" or Emotion == \"Amused\" or Emotion == \"Sleepiness\":\n",
        "  checkpoint = torch.load('trained_models/'+Emotion.lower()+'_tacotron.pth', map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "  wav, align, mel = tts_tacotron(model, Text)\n",
        "\n",
        "present(wav, Emotion, (align,mel))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AchHD0xLcaE1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Demo - DL Based Emotional TTS",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}